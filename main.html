<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Home Page</title>
    <link rel="stylesheet" href="stylesheet.css">
    <script src="http://code.jquery.com/jquery-2.1.4.min.js"></script>
    <script>
    $(function(){$('a').each(function () {
        if ($(this).prop('href') == window.location.href) {
            $(this).addClass('active'); $(this).parents('li').addClass('active');
        }
    })});
</script>
</head>
<body>

    <ul class="navbar">
        <li><a href="main.html" >Home<br>Page</a></li>
        <li><a href="enviroment.html"  >Enviromental<br>Impacts</a></li>
        <li><a href="medical.html"  >Medical<br>Benefits</a></li>
        <li><a href="economic.html" >Economic<br>Impacts</a></li>
        <li><a href="references.html"  >References<br>List</a></li>
        <li><a href="activity.html"  >Activity<br>Log</a></li>
    </ul>




    <div class="fullWidthImage">
        
        <img src="datacenter.png" alt="an image of a datacenter">
        <p class="centeredTitle">
            The Enviromental Impacts of Generative AI
        </p>
    </div>



    <div class="gridTitle">
        <p>
            How does Generative AI work?
        </p>
    </div>



    <div class="twoColumnGrid">

        <div class="column">
            <p>

               In order to create Generative AI, researchers create a “foundational model”. <a href="references.html#ref1How">(1)</a> 
               This is a Large Language Model (LLM) trained on a very broad set of data. <br><br> For the purposes of this explanation, 
               this case study will be focusing on text-based Generative AI, but the same principles hold true for other variants.

            </p>
        </div>


        <div class="columnImage">
            <img src="types-of-llms_3.png" alt="a timeline showing when certain LLMs were created">
        </div>  

        <div class="column">
            <p>
                In order to train a foundational model, researchers use Deep Learning algorithms on vast amounts of raw, unprocessed data . 
                The deep learning algorithm is trained by performing “fill in the blank” exercises, where it uses a neural network to attempt 
                to predict the blanked-out word <a href="references.html#ref1How">(1)</a>.<br><br> Neural Networks have a series of parameters that they receive, 
                and a series of values that 
                they output, called the Input and Output layers respectively. In-between these layers are any number of Hidden Layers. These layers 
                consist of nodes which perform various operations on the data. <br><br>
                The image opposite shows a high-level diagram of the construction of a neural network:
            </p>
        </div>
        <div class="columnImage">

            <img src="neuralnet.png" alt="a diagram showing the structure of a neural net">

        </div>
        <div class="columnImage">
            <img src="types-of-llms_1.png" alt="an image showing the various building blocks of LLMs">
        </div>
        <div class="column">
            <p>
            The key to understanding how neural networks are trained is to realise that these 
            connections and values are subject to change- when the deep learning algorithm makes a 
            prediction, it is given a score value based on how accurate the prediction was. The neural 
            net then back-propagates the error through its network, and each node calculates the effect 
            that its values had on that error. The values are then adjusted to reduce the error, and the 
            process is repeated. Over many, many iterations, this allows the neural net to become more 
            proficient at its task. <a href="references.html#ref2How">(2)</a> <br><br>
            The result of all of this training is a neural network capable of recognising patterns in data. If properly configured, 
            this neural network can expand upon its own output, allowing it to create novel works autonomously.
            </p>
        </div>

        <div class="column">
            <p>
                
                With a foundational model developed, specialisation can begin. The foundational model is a generalist- it has learned how to analyse 
                and generate text, but it cannot generate reliable, specific information with high fidelity or accuracy. <br><br> In order to remedy this, AI 
                goes through a fine-tuning process. The model is given both the input data (prompts) and the desired output data. This data is labelled 
                and categorised so that the model can more efficiently evaluate the cause of the desired output. This approach is very labour intensive, 
                as it requires the manual labelling of large amounts of data to train the model.<a href="references.html#ref1How">(1)</a><br>
                The diagram opposite shows a high-level view of how this training approach works.
            </p>
        </div>
        <div class="columnImage">
            <img src="hrefl.png" alt="an image showing the process of Human Re-enforced Learning">
        </div>
    </div>
   
</body>
</html>